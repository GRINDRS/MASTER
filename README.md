# Museum Robot Guidance & Experience Platform

This repository contains an **end-to-end system** that turns a mobile robot into an engaging museum guide.  
It combines Natural-Language interaction, autonomous navigation, computer-vision utilities, and a real-time web dashboardâ€”all connected through MQTT.

---

## 1.â€ƒHigh-Level Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           MQTT            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  nlp_voice_bot/voicebot  â”‚â”€â”€movementâ”€â–¶â”‚ navigation/navigation.py â”‚
â”‚    (speech â†”ï¸ GPT-4)      â”‚â—€â”€arrivedâ”€â”€â”€â”‚   (PLC / SLAM layer)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                                     â”‚ (optionally)
              â”‚  JSON file (spoke.json)             â”‚
              â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           HTTP           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    web/app.py (Flask)    â”‚â—€â”€â”€â”€â”€â”€â”€â”€browserâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     Video Stream + UI      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
* **Voice Bot** â€“ Handles speech recognition, exhibit selection logic, Q&A (powered by OpenAI), and publishes navigation commands.
* **Navigation Layer** â€“ Subscribes to movement commands, drives the robot (or a simulation) and publishes an `arrived` message when it reaches its goal.
* **Web Dashboard** â€“ Displays the current conversation, exhibit itinerary, live webcam feed, and provides at-a-glance status for operators or visitors.
* **Shared JSON (`web/spoke.json`)** â€“ A simple hand-off mechanism between the voice bot and the web UI; keeps the dashboard in sync without a full database.

---

## 2.â€ƒDirectory Overview
```
.
â”œâ”€â”€ basic_embedded/                # Micro-controller prototypes (C / Arduino)
â”œâ”€â”€ computer_vision_gpt_approach/  # Exploratory CV+GPT scripts
â”œâ”€â”€ navigation/
â”‚   â””â”€â”€ navigation.py              # MQTT-driven movement simulation / hardware bridge
â”œâ”€â”€ nlp_voice_bot/
â”‚   â”œâ”€â”€ voicebot.py                # Main conversational agent
â”‚   â”œâ”€â”€ test_mqtt.py               # Simple publisher for manual tests
â”‚   â””â”€â”€ README.md                  # Notes specific to the bot
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ app.py                     # Flask server
â”‚   â”œâ”€â”€ templates/index.html       # Dashboard HTML (Jinja2)
â”‚   â””â”€â”€ spoke.json                 # Shared state (auto-generated)
â”œâ”€â”€ start_museum_system.sh         # Convenience script to launch components
â”œâ”€â”€ docker-compose.yml             # Multi-container deployment (MQTT + services)
â”œâ”€â”€ Dockerfile                     # Base image for Python services
â”œâ”€â”€ requirements.txt               # All Python dependencies
â””â”€â”€ README.md                      # â† you are here
```

---

## 3.â€ƒComponent Details

### 3.1â€ƒ`nlp_voice_bot/voicebot.py`
* **Speech I/O** â€“ Uses `speech_recognition` + Google STT for input, `gTTS` + `ffplay` for output.
* **State Machines** â€“ Supports two visit modes:
  * **Guided Tour** â€“ A fixed three-piece curriculum (Mona Lisa â†’ Starry Night â†’ The Scream).
  * **Free Roam** â€“ The visitor picks any exhibit, can say *next* or *stop* at any time.
* **OpenAI Integrations**  
  * `exhibit_summary()` â€“ Condenses background into 2-3 sentences.  
  * `answer_question()` â€“ On-the-fly Q&A about the current piece.
* **Error Handling** â€“     
  * Time-outs for movement acknowledgement.  
  * "Sorry, I didn't catch that" reprompts on STT failures or unclear intent.
* **Shared Dashboard State** â€“ Updates `web/spoke.json` whenever the user or bot speaks, and when the itinerary changes.

### 3.2â€ƒ`navigation/navigation.py`
* **MQTT Topics** â€“ Listens on `movement`; publishes on `arrived`.
* **Simulation vs Hardware** â€“ Default behaviour sleeps ~5 s to mimic travel.  Swap the placeholder with actual motor/SLAM calls.

### 3.3â€ƒ`web/app.py`
* **Flask Routes**  
  `/` â€“ Renders the dashboard with current JSON.  
  `/video` â€“ MPEG stream from an attached webcam.  
  `/frame.jpg` â€“ Snapshot endpoint.
* **Front-End** â€“ The template shows:
  * Latest user utterance
  * Bot response
  * Current exhibit
  * Visited & upcoming lists
  * Live video panel (uses HTML `<img>` stream)

### 3.4â€ƒShell & Orchestration
* **`start_museum_system.sh`** â€“ Convenience script that:
  1. Starts Mosquitto (if not already running)
  2. Launches navigation, voice bot and web server in dedicated terminals.
* **Docker Compose** â€“ Spins up: Mosquitto, navigation, voice bot, and web dashboard in isolated containers.

---

## 4.â€ƒGetting Started (Local Machine)
```bash
# 1.  Python env + system libs
brew install ffmpeg mosquitto
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2.  Secrets
cp .env.example .env         # then add your OPENAI_API_KEY

# 3.  Launch everything
./start_museum_system.sh     # or run each component manually

# 4.  Open the dashboard
open http://localhost:5000
```

---

## 5.â€ƒEnvironment Variables
| Variable            | Purpose                       |
|---------------------|-------------------------------|
| `OPENAI_API_KEY`    | Access token for GPT requests |
| `MQTT_BROKER`       | Hostname (default `localhost`) |
| `MQTT_PORT`         | Port (default `1883`)         |

---

## 6.â€ƒExtending Exhibits
Edit the list in **`voicebot.py â†’ EXHIBITS`**:
```python
EXHIBITS = [
    {"keyword": "lilies",   "location": "Water Lilies by Claude Monet"},
    # add more â€¦
]
```
Each entry needs:
* **`keyword`** â€“ what the visitor might say.
* **`location`** â€“ the friendly display & navigation target.

---

## 7.â€ƒFAQ / Troubleshooting
* **The bot hangs on navigation** â€“ Check that `navigation/navigation.py` publishes `arrived` or increase the time-out in `wait_for_arrival()`.
* **Speech recognition errors** â€“ Ensure your microphone is correctly configured; the bot already reprompts on failure.
* **OpenAI 400 error** â€“ Make sure text sent to GPT is non-empty (fixed in latest patch).

---

## 8.â€ƒLicence
No licensing required except for the openAI key.

---

## 9.â€ƒDevelopment Workflow

Below is the **recommended process** for contributing new features or debugging existing ones.

1. **Fork & Clone** the repository, then create a feature branch.
2. **Spin-up required services locally** using `docker-compose up mqtt` to ensure a broker is running.
3. **Run unit tests** (where available) via `pytest -q`â€”see section 13 for how tests are structured.
4. **Start the live system** in *watch mode*:
   ```bash
   ./start_museum_system.sh dev   # spawns each component with live-reload
   ```
5. **Tail the logs** for each service (navigation, voicebot, web) in their respective terminals.
6. **Edit code**; changes to `voicebot.py`, templates, or `navigation.py` will automatically reload thanks to the built-in watchers.
7. **Commit & push** once the feature is verified; open a PR and link to any relevant issues.

---

## 10.â€ƒDirectory Deep Dive

| Path | Purpose |
|------|---------|
| `basic_embedded/` | Experiments for driving stepper motors with Arduino/C++; may inform future hardware integration. |
| `computer_vision_gpt_approach/` | Proof-of-concept scripts that describe a camera frame to GPT for object recognition; not used in production yet but demonstrates multi-modal potential. |
| `capture_analyse.py` | Quick utility to snap webcam frames and analyse facial sentimentâ€”another CV exploration. |
| `navigation/` | All motion-related logic; right now a pure-Python simulator but intended to be swapped with ROS2 or PLC code. |
| `nlp_voice_bot/` | Speech, NLP, GPT calls, itinerary logic, and JSON syncing. |
| `web/` | Lightweight dashboard (Flask + Jinja2) that surfaces system state and video feed. |
| `docker-compose.yml` | Defines 4 services: `mqtt`, `voicebot`, `navigation`, and `dashboard`. |
| `start_museum_system.sh` | Bash script to orchestrate local development (with automatic process restarts). |

---

## 11.â€ƒMQTT Topic Specification

| Topic | Direction | Payload | Description |
|-------|-----------|---------|-------------|
| `movement` | VoiceBot â†’ Navigation | *string* (exhibit name) | Tells the robot to navigate to a particular exhibit or waypoint such as `"Mona Lisa by Leonardo da Vinci"`. |
| `arrived` | Navigation â†’ VoiceBot | *string* (echo of location) | Signals that navigation is completeâ€”VoiceBot unblocks its flow once this is received. |

QoS 0 is sufficient for a small venue; consider QoS 1 if the network is spotty.

---

## 12.â€ƒSpeech & LLM Configuration

| Component | Lib | Notes |
|-----------|-----|-------|
| **STT** | `speech_recognition` (Google Web Speech) | You can swap in Offline Vosk by setting `STT_ENGINE=vosk` in `.env`. |
| **TTS** | `gTTS` + `ffplay` | Uses FFmpeg's playback to allow variable speed via `-af atempo=1.3`. |
| **LLM** | OpenAI Chat Completions | Model selectable via `OPENAI_MODEL` (defaults to `gpt-3.5-turbo`). |

If you need full offline mode, replace TTS with `espeak` and LLM calls with a local model such as `llama.cpp`â€”the interfaces are isolated in helper functions to ease substitution.

---

## 13.â€ƒTesting Strategy

* **Unit Tests (`tests/`)** â€“ Cover helper functions like `choose_locs`, intent detection, and navigation fallbacks.
* **Integration Scenario** â€“ A scripted conversation fed through a mocked STT layer ensures the full stack responds as intended; see `tests/test_convo.py`.
* **Hardware-in-the-loop** â€“ When a real robot is available, the navigation simulator is disabled via `NAV_SIM=false` and tests verify physical arrival sensors.

To run all tests:
```bash
pytest -q
```

---

## 14.â€ƒDeployment Targets

| Platform | Tested | Notes |
|----------|--------|-------|
| **Docker on x86-64** | âœ… | Fastest way to replicate full environment. |
| **Raspberry Pi 4** | âœ… | Requires `sudo apt install ffmpeg mosquitto` and enabling pi camera. |
| **Jetson Nano** | âš ï¸ | Builds fine but GPU CV demo scripts require patched OpenCV. |
| **ROS 2 Foxy** | ğŸš§ | Planned: wrap navigation publisher/subscriber as ROS nodes. |

---

## 15.â€ƒContribution Guidelines

* Follow **PEP-8** and document public functions with *Google-style* docstrings.
* Run `ruff --fix .` before committing to auto-format and lint.
* Each new exhibit or feature should be accompanied by at least one test.
* File an issue first before embarking on large architectural changes.

---

## 16.â€ƒRoadmap / Future Work

1. **Dynamic map** â€“ Integrate SLAM feedback into the dashboard to show real-time robot position.
2. **Multi-language support** â€“ Parameterise TTS/STT to switch languages on the fly.
3. **Crowd analytics** â€“ Use the computer-vision module to estimate foot-traffic and adjust tour suggestions.
4. **Ticketing system integration** â€“ Personalise tours based on visitor profile read from QR code.
5. **Accessibility mode** â€“ Provide on-screen captions and sign-language avatar.

---
